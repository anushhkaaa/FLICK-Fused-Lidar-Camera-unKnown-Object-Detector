# Fused-Lidar-Camera-unKnown-Object-Detector
Detecting unknown objects in real-time is crucial for the safety, adaptability, and overall effectiveness of autonomous vehicles on the road. The ability to recognize and respond to unforeseen obstacles is essential for ensuring the safety of vehicle occupants, pedestrians, and other road users. Autonomous vehicles must be capable of adapting to dynamic environments, such as unexpected debris, construction zones, or unusual road conditions. This adaptability contributes to enhanced decision-making, allowing the vehicle to navigate complex urban environments and avoid collisions with objects not encountered during training. The robustness of autonomous systems is strengthened by their capability to handle unknown objects, meeting regulatory requirements and instilling confidence in users and regulators. In summary, the detection of unknown objects is a fundamental aspect of autonomous vehicle technology, contributing to safety, adaptability, and overall reliability in real-world driving scenarios.

On that note, developing a Fused Lidar Camera Unknown Object Detector will greatly enhance the perception of autonomous vehicles. To do so, I used the NuScenes dataset and based my work on the **BEVFusion** framework -- an efficient multi-sensor fusion framework. It fuses independent features from Camera and LiDAR, uses separate 2D and 3D backbones for them respectively, and projects them onto the same shared Bird's Eye View space to preserve geometric and semantic information. In this way, it removes the bottlenecks of state-of-the-art point level fusion, in which the semantic density of camera features gets severely compromised in case of a LiDAR failure or if the LiDAR point clouds are not accurate. 

The primary objective of this project is to design and add new application heads on the chosen backbones for Camera and LiDAR respectively, to generate BEV Map Segmentation, 3D detection and ultimately the 3D classification of "unknown" objects in real time. 
